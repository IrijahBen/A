{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11650,
          "sourceType": "datasetVersion",
          "datasetId": 8327
        },
        {
          "sourceId": 59255,
          "sourceType": "datasetVersion",
          "datasetId": 38891
        },
        {
          "sourceId": 239192,
          "sourceType": "datasetVersion",
          "datasetId": 100982
        },
        {
          "sourceId": 320111,
          "sourceType": "datasetVersion",
          "datasetId": 134715
        },
        {
          "sourceId": 2510329,
          "sourceType": "datasetVersion",
          "datasetId": 1520310
        },
        {
          "sourceId": 4368115,
          "sourceType": "datasetVersion",
          "datasetId": 1152384
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "üîµCyberbullying tweetsüò≠ EDA | AutoML | DL | BERT",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrijahBen/A/blob/main/%F0%9F%94%B5Cyberbullying_tweets%F0%9F%98%AD_EDA_%7C_AutoML_%7C_DL_%7C_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "takuok_glove840b300dtxt_path = kagglehub.dataset_download('takuok/glove840b300dtxt')\n",
        "vkrahul_twitter_hate_speech_path = kagglehub.dataset_download('vkrahul/twitter-hate-speech')\n",
        "arkhoshghalb_twitter_sentiment_analysis_hatred_speech_path = kagglehub.dataset_download('arkhoshghalb/twitter-sentiment-analysis-hatred-speech')\n",
        "lakshmi25npathi_imdb_dataset_of_50k_movie_reviews_path = kagglehub.dataset_download('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')\n",
        "jp797498e_twitter_entity_sentiment_analysis_path = kagglehub.dataset_download('jp797498e/twitter-entity-sentiment-analysis')\n",
        "saurabhshahane_cyberbullying_dataset_path = kagglehub.dataset_download('saurabhshahane/cyberbullying-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "rKP7-4UKpaVg",
        "outputId": "fc934ca5-99ee-4ac9-e7ce-7f871c0445b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/takuok/glove840b300dtxt?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.08G/2.08G [01:20<00:00, 27.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vkrahul/twitter-hate-speech?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.89M/1.89M [00:00<00:00, 2.35MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arkhoshghalb/twitter-sentiment-analysis-hatred-speech?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.89M/1.89M [00:00<00:00, 2.33MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.7M/25.7M [00:01<00:00, 16.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jp797498e/twitter-entity-sentiment-analysis?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.99M/1.99M [00:00<00:00, 2.44MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/saurabhshahane/cyberbullying-dataset?dataset_version_number=8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64.9M/64.9M [00:03<00:00, 18.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/hundreds-of-twitter-logos.png\"\n",
        "     width=\"300000000\" />\n",
        "\n",
        "<h1 style='font-family: Source Sans Pro'>Cyberbullying Detection EDA | AutoML | DL | BERT</h1>\n",
        "\n",
        "This is a subtle attempt at learning the basics of NLP, Text analysis, experimenting with DL models and State of the art attention models like BERT. We start with an in depth analysis of the tweets that we have, preprocess them and go forward with the modelling process.\n",
        "\n",
        "We have multiple datasets containing data from multiple sources like youtube, twitter, kaggle, etc. Every dataset will have seperate preprocessing steps. We're going with the `twitter_parsed_dataset` for the sake of simplicity.\n",
        "Throughout the notebook, I'll try to take you through my though process as a beginner in NLP. I'll dive into all the challenges I faced and solutions found. lets begin!!\n",
        "\n",
        "<h2 style='font-family: Source Sans Pro'>Table of contents</h2>\n",
        "\n",
        "[Importing the necessary libraries ](#a1)<br>\n",
        "[Basic preprocessing and sample tweets](#a2)\n",
        "\n",
        "* [üïµüèª Exploratory Data Analysis](#chapter1)\n",
        "    * [Sentiment analysis](#section_1_1)\n",
        "    * [Analysis of common hashtags and mentions](#section_1_2)\n",
        "    * [‚òÅÔ∏è Wordclouds for toxic text](#section_1_3)\n",
        "\n",
        "\n",
        "* [üëæ Modelling](#chapter2)\n",
        "    * [ML approach with word2vec](#section_2_1)\n",
        "        * [Pycaret's AutoML](#section_2_1_1)\n",
        "    * [üß† Deep learning models](#section_2_2)\n",
        "        * [‚ôæÔ∏è Simple RNNs with trainable embeddings](#section_2_2_1)\n",
        "        * [üåê LSTM with GloVe embeddings](#section_2_2_2)\n",
        "        * [GRUs with trainable embeddings](#section_2_2_3)\n",
        "    * [‚¨ÖÔ∏è‚û°Ô∏è BERT with Tensorflow Hub](#section_2_3)\n",
        "        * [How to use a tf hub BERT model](#section_2_3_1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YNCz-V5upaVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîµ Installing and importing libraries <a class=\"anchor\"  id=\"a1\"></a>"
      ],
      "metadata": {
        "id": "n6dQf6QJpaVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "! pip install -Uqqq seaborn\n",
        "! pip install -Uqqq squarify\n",
        "! pip install -Uqqq nltk\n",
        "! pip install -Uqqq wordcloud\n",
        "! pip install -Uqqq spacy\n",
        "! pip install -Uqqq imblearn\n",
        "! pip install -Uqqq gensim\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "8vcJCtbKpaVk",
        "outputId": "4640033e-bfe8-42d2-b5ea-2b94dc5c4cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imread\n",
        "import requests\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import squarify\n",
        "from tqdm import tqdm\n",
        "\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "import spacy\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU,SimpleRNN\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-14T18:04:05.62165Z",
          "iopub.execute_input": "2024-01-14T18:04:05.622282Z",
          "iopub.status.idle": "2024-01-14T18:04:40.695226Z",
          "shell.execute_reply.started": "2024-01-14T18:04:05.622254Z",
          "shell.execute_reply": "2024-01-14T18:04:40.694293Z"
        },
        "trusted": true,
        "id": "VK_-xpAfpaVk",
        "outputId": "ef470f40-bea2-4fc4-b9ba-0dc65399cab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'text' from 'keras.preprocessing' (/usr/local/lib/python3.11/dist-packages/keras/api/preprocessing/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-693f55b7e8d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'text' from 'keras.preprocessing' (/usr/local/lib/python3.11/dist-packages/keras/api/preprocessing/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-14T18:11:45.001062Z",
          "iopub.execute_input": "2024-01-14T18:11:45.00142Z",
          "iopub.status.idle": "2024-01-14T18:11:45.012549Z",
          "shell.execute_reply.started": "2024-01-14T18:11:45.00139Z",
          "shell.execute_reply": "2024-01-14T18:11:45.011644Z"
        },
        "trusted": true,
        "id": "Ec_1h3WFpaVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "\n",
        "def add_spines(colour = '#425169', linewidth = 2):\n",
        "    \"\"\"\n",
        "    Add beautiful spines to you plots\n",
        "    \"\"\"\n",
        "    ax = plt.gca()\n",
        "    ax.spines['top'].set_visible(True)\n",
        "    ax.spines['right'].set_visible(True)\n",
        "    ax.spines[['bottom', 'left', 'top', 'right']].set_color(colour)\n",
        "    ax.spines[['bottom', 'left', 'top', 'right']].set_linewidth(linewidth)\n",
        "\n",
        "def roc_auc(predictions,target):\n",
        "    '''\n",
        "    This methods returns the AUC Score when given the Predictions\n",
        "    and Labels\n",
        "    '''\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:33.917468Z",
          "iopub.execute_input": "2024-01-14T18:13:33.917837Z",
          "iopub.status.idle": "2024-01-14T18:13:33.925053Z",
          "shell.execute_reply.started": "2024-01-14T18:13:33.917808Z",
          "shell.execute_reply": "2024-01-14T18:13:33.92404Z"
        },
        "trusted": true,
        "id": "VVtw_GsVpaVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîµ Basic preprocessing and sample tweets <a class=\"anchor\"  id=\"a2\"></a>"
      ],
      "metadata": {
        "id": "K4u0NrzQpaVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\n",
        "df = df.drop(['id', 'index'], axis=1)\n",
        "df.rename(columns={'oh_label': 'Label'}, inplace=True)\n",
        "df = df.dropna(subset=['Label'])\n",
        "\n",
        "# analysis of mentions and tweets\n",
        "\n",
        "# here we create a function to count the number of hashtags and mentions and then create two columns to store this data\n",
        "def count_symbols(text):\n",
        "    hashtag_count = len(re.findall(r'#', text))\n",
        "    mention_count = len(re.findall(r'@', text))\n",
        "    return hashtag_count, mention_count\n",
        "\n",
        "# Apply the function to each row in the 'text' column\n",
        "df[['num_hashtags', 'num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n",
        "\n",
        "print('\\nDataset shape: ', df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:44:10.465072Z",
          "iopub.execute_input": "2024-01-14T15:44:10.465477Z",
          "iopub.status.idle": "2024-01-14T15:44:14.647079Z",
          "shell.execute_reply.started": "2024-01-14T15:44:10.465446Z",
          "shell.execute_reply": "2024-01-14T15:44:14.645695Z"
        },
        "trusted": true,
        "id": "Qj-P4oIvpaVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('_________Sample tweets_________')\n",
        "simple_tweet = df[df['Label'] == 0]['Text'].iloc[1]\n",
        "print('\\nSimple tweet: ', simple_tweet)\n",
        "sexist_tweet = df[df['Annotation'] == 'sexism']['Text'].iloc[0]\n",
        "print('\\n\\nSexist tweet: ', sexist_tweet)\n",
        "racist_tweet = df[df['Annotation'] == 'racism']['Text'].iloc[0]\n",
        "print('\\n\\nRacist tweet: ', racist_tweet)\n",
        "print('\\n_______________________________')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:44:14.649505Z",
          "iopub.execute_input": "2024-01-14T15:44:14.650973Z",
          "iopub.status.idle": "2024-01-14T15:44:14.681195Z",
          "shell.execute_reply.started": "2024-01-14T15:44:14.650909Z",
          "shell.execute_reply": "2024-01-14T15:44:14.680179Z"
        },
        "trusted": true,
        "id": "QKTpasfdpaVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üïµüèª Exploratory Data Analysis <a class=\"anchor\"  id=\"chapter1\"></a>\n",
        "\n",
        ">We analyse different aspects of the twitter dataset here. We start with simple tasks like understanding the label and Annotation distribution, and slowly progress toward extracting and analysing #hastags, @mentions and finally generate wordclouds of high frequency words according to annotations."
      ],
      "metadata": {
        "id": "uJaoHGFYpaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Basic analysis of labels and annotations\n",
        "\n",
        "print('Label distribution')\n",
        "print(df.Label.value_counts())\n",
        "print('-------------------------\\n')\n",
        "print('Annotation distribution')\n",
        "print(df.Annotation.value_counts())\n",
        "print('-------------------------\\n')\n",
        "print('Grouping of Annotation with label')\n",
        "print(df.groupby('Annotation')['Label'].sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:13.14697Z",
          "iopub.execute_input": "2024-01-14T15:45:13.147558Z",
          "iopub.status.idle": "2024-01-14T15:45:13.180593Z",
          "shell.execute_reply.started": "2024-01-14T15:45:13.147512Z",
          "shell.execute_reply": "2024-01-14T15:45:13.179348Z"
        },
        "trusted": true,
        "id": "msq1rmvgpaVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the text with annotations of anything other than none are all classified as toxic. Hence, it would be a dead giveaway to include it in our dataset. We may try to classify the text with the annotations later\n",
        "___"
      ],
      "metadata": {
        "id": "GfIhXpOApaVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Highlighting columns in your seaborn plots<br>\n",
        "This is just a fun way in which you can highlight certain bars in your barplot. We create a specific function that returns a palette of length equal to the unique categories in the column you're plotting and specifically highlights the indexes passed to it."
      ],
      "metadata": {
        "id": "OZNcZRgHpaVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bully_palette(df,column, positions_to_change: list):\n",
        "    \"\"\"\n",
        "    A function to create grey red palettes according to the inputs\n",
        "    You just need to pass in the dataframe and the index of labels to be highlighted in red\n",
        "    \"\"\"\n",
        "    palette = ['#96898b']*df[column].nunique()\n",
        "    new_values = ['#cc253b']*len(positions_to_change)\n",
        "    for position, new_value in zip(positions_to_change, new_values):\n",
        "        palette[position] = new_value\n",
        "    return sns.color_palette(palette)\n",
        "\n",
        "# example\n",
        "# bully_palette(df,'Label', [0, 1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:19.141835Z",
          "iopub.execute_input": "2024-01-14T15:45:19.143668Z",
          "iopub.status.idle": "2024-01-14T15:45:19.150812Z",
          "shell.execute_reply.started": "2024-01-14T15:45:19.143616Z",
          "shell.execute_reply": "2024-01-14T15:45:19.149195Z"
        },
        "trusted": true,
        "id": "MUvdYBVWpaVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(6, 2))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# using the bully_pallete function to create a custom pallete\n",
        "sns.countplot(x='Label', data=df, palette = bully_palette(df, 'Label', [1]))\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Labels')\n",
        "# adding spines using the helper function we declared earlier\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(x='Annotation', data=df, palette = bully_palette(df,'Annotation', [1, 2]))\n",
        "plt.xlabel('Annotation')\n",
        "plt.ylabel('')\n",
        "plt.title('Distribution of Annotation')\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.4, hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:40.126534Z",
          "iopub.execute_input": "2024-01-14T15:45:40.126978Z",
          "iopub.status.idle": "2024-01-14T15:45:40.647613Z",
          "shell.execute_reply.started": "2024-01-14T15:45:40.126942Z",
          "shell.execute_reply": "2024-01-14T15:45:40.64636Z"
        },
        "trusted": true,
        "id": "MgwwbLvppaVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how our highlighted plots turned out. Do play around with these... highlight with your own colours... add functionalities etc.<br><br>\n",
        "We can see that the lable `1` has further been divided into the categories `sexism` and `racism`. Sexism is more prevelant than racism. Racism is looked upon more harshly by the public or twitter might be efficient at removing racist tweets from their site (we never knowü§∑‚Äç‚ôÄÔ∏è)."
      ],
      "metadata": {
        "id": "haOX7ZqRpaVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 3))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x='num_mentions', data=df, palette =  sns.color_palette(\"mako\", n_colors=9)[3:])\n",
        "plt.xlabel('No of mentions')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Mentions (@)')\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(x='num_hashtags', data=df, palette = sns.color_palette(\"mako\", n_colors=11)[5:])\n",
        "plt.xlabel('No of hashtags')\n",
        "plt.ylabel('')\n",
        "plt.title('Distribution of Hashtags (#)')\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.23, hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:44.436404Z",
          "iopub.execute_input": "2024-01-14T15:45:44.436851Z",
          "iopub.status.idle": "2024-01-14T15:45:45.163114Z",
          "shell.execute_reply.started": "2024-01-14T15:45:44.436815Z",
          "shell.execute_reply": "2024-01-14T15:45:45.161603Z"
        },
        "trusted": true,
        "id": "xd5IsAkZpaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 3))\n",
        "mention_label_cross = pd.crosstab(df['num_mentions'], df['Label'])\n",
        "mention_label_cross['sum'] = mention_label_cross[0.0] + mention_label_cross[1.0]\n",
        "mention_label_cross['non bullying tweet %'] = mention_label_cross[0.0]/mention_label_cross['sum']*100\n",
        "mention_label_cross['bullying tweet %'] = mention_label_cross[1.0]/mention_label_cross['sum']*100\n",
        "mention_label_cross = mention_label_cross.drop([0.0, 1.0, 'sum'], axis=1)\n",
        "\n",
        "mention_label_cross.plot(kind='bar', stacked=True, figsize=(6, 4), color=['#6dd2ac', '#3487a5'], width=0.8)\n",
        "plt.xlabel('No of mentions')\n",
        "plt.ylabel('percentage %')\n",
        "plt.title('Relation between no.of mentions (@) and bullying')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:45.899913Z",
          "iopub.execute_input": "2024-01-14T15:45:45.900376Z",
          "iopub.status.idle": "2024-01-14T15:45:46.394181Z",
          "shell.execute_reply.started": "2024-01-14T15:45:45.900342Z",
          "shell.execute_reply": "2024-01-14T15:45:46.39332Z"
        },
        "trusted": true,
        "id": "7HbLsMAQpaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Here you can see that as the mentions increase to 4, the percentage of bullying tweets increases compared to the normal ones. But the trend once again reverses as the mentions increase upto 8. No clear pattern was seen between number of hashtags and percentage of bullying tweets"
      ],
      "metadata": {
        "id": "cG_d2epxpaVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment analysis\n",
        "Here I planned on checking whether the sentiment of the tweets directly correlate with bullying. ie. more  negativity a tweet contains, higher the probability of bullying. Turns out it wasn't as straightforward as that. The dataset contains a lot of negative tweets... which is a good thing since our models should be able to understand the difference between a `negative` tweet and a `bullying` tweet. It shouldn't flag every other negative tweet as bullying."
      ],
      "metadata": {
        "id": "vk8eoEfxpaVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Using the nltk library to analyze sentiment of each text so that we can correlate it with bullying\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "def get_sentiment(text):\n",
        "    compound_score = sia.polarity_scores(text)['compound']\n",
        "    return 'positive' if compound_score >= 0 else 'negative'\n",
        "\n",
        "\n",
        "df['Sentiment_Label'] = df['Text'].apply(get_sentiment)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:50.871371Z",
          "iopub.execute_input": "2024-01-14T15:45:50.871835Z",
          "iopub.status.idle": "2024-01-14T15:45:56.148199Z",
          "shell.execute_reply.started": "2024-01-14T15:45:50.871791Z",
          "shell.execute_reply": "2024-01-14T15:45:56.146795Z"
        },
        "trusted": true,
        "id": "W74CNA6dpaVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "df['Sentiment_Label'].value_counts()\n",
        "sns.countplot(x='Sentiment_Label', data=df, palette = bully_palette(df, 'Sentiment_Label', [0]))\n",
        "plt.title('Count of sentiment in text')\n",
        "plt.xlabel('')\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "sent_label_cross = pd.crosstab(df['Sentiment_Label'], df['Label'])\n",
        "sent_label_cross['sum'] = sent_label_cross[0.0] + sent_label_cross[1.0]\n",
        "sent_label_cross['non bullying tweet %'] = sent_label_cross[0.0]/sent_label_cross['sum']*100\n",
        "sent_label_cross['bullying tweet %'] = sent_label_cross[1.0]/sent_label_cross['sum']*100\n",
        "sent_label_cross = sent_label_cross.drop([0.0, 1.0, 'sum'], axis=1)\n",
        "\n",
        "ax = sent_label_cross.plot(kind='bar', stacked=True, figsize=(6, 4), color=['#6dd2ac', '#3487a5'], width=0.8)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('percentage %')\n",
        "plt.title('Relation between sentiment and bullying')\n",
        "ax.legend(loc='lower right')\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:45:58.085707Z",
          "iopub.execute_input": "2024-01-14T15:45:58.086225Z",
          "iopub.status.idle": "2024-01-14T15:45:58.683797Z",
          "shell.execute_reply.started": "2024-01-14T15:45:58.086182Z",
          "shell.execute_reply": "2024-01-14T15:45:58.682262Z"
        },
        "trusted": true,
        "id": "Q-kS0DCKpaVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>üí°Idea:</b> We could add either the sentiment score or the label as a feature to train our models. Though I haven't actually done that... it may be a great idea for you guys to try out!!"
      ],
      "metadata": {
        "id": "yve4297wpaVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of common hashtags and mentions\n",
        "\n",
        "The tweets contain a bunch of hashtags and mentions. What kind of twitter analysis would this be if we leave these out :P  Analysing this part made me have questions over the quality of the dataset. Let's understand why...\n",
        "- The tweets seems to be majorly sourced from particular feeds or are retweets to particular user.\n",
        "- A majority of the tweets feature the TV show `My kitchen rules`.\n",
        "\n",
        "#### A very key takeaway:\n",
        "The data collector was an australian interested in cookingüë®‚Äçüç≥üòÜ"
      ],
      "metadata": {
        "id": "qgwdo7J2paVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Extraction of the top ten mentions and hashtags in the dataset\n",
        "\n",
        "# Function to extract hashtags from a text\n",
        "def extract_hashtags(text):\n",
        "    return re.findall(r'#\\w+', text)\n",
        "\n",
        "df['Hashtags'] = df['Text'].apply(extract_hashtags)\n",
        "\n",
        "# Function to extract hashtags from a text\n",
        "def extract_mentions(text):\n",
        "    return re.findall(r'@\\w+', text)\n",
        "\n",
        "df['Mentions'] = df['Text'].apply(extract_mentions)\n",
        "\n",
        "def create_counter_df(df):\n",
        "    \"\"\"\n",
        "    Creates a dataframe that contains the Label and their count.\n",
        "    Pass the dataframe containing the labels in the form of a list column as an input.\n",
        "    Example usage:\n",
        "    1. mentions = create_counter_df(df['Mentions'])\n",
        "    2. sexist_mentions = create_counter_df(df[df.Annotation=='sexist'][\"Mentions\"])\n",
        "    \"\"\"\n",
        "    # Flatten the list of hashtags and count their occurrences\n",
        "    all_counts = [tag for counts_list in df for tag in counts_list]\n",
        "    label_counts = Counter(all_counts)\n",
        "\n",
        "    # Create a DataFrame from the Counter dictionary\n",
        "    counts_df = pd.DataFrame(list(label_counts.items()), columns=['Label', 'Count'])\n",
        "    counts_df = counts_df.sort_values(by='Count', ascending=False)\n",
        "    return counts_df\n",
        "\n",
        "mentions_df = create_counter_df(df['Mentions'])\n",
        "top_10_mentions = mentions_df.head(10)\n",
        "sexist_mentions = create_counter_df(df[df.Annotation=='sexism'][\"Mentions\"])\n",
        "top_10_sexist_mentions = sexist_mentions.head(10)\n",
        "racist_mentions = create_counter_df(df[df.Annotation=='racism'][\"Mentions\"])\n",
        "top_10_racist_mentions = racist_mentions.head(10)\n",
        "\n",
        "hashtags_df = create_counter_df(df['Hashtags'])\n",
        "top_10_hashtags = hashtags_df.head(10)\n",
        "sexist_hashtags = create_counter_df(df[df.Annotation=='sexism'][\"Hashtags\"])\n",
        "top_10_sexist_hashtags = sexist_hashtags.head(10)\n",
        "racist_hashtags = create_counter_df(df[df.Annotation=='racism'][\"Hashtags\"])\n",
        "top_10_racist_hashtags = racist_hashtags.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:46:06.598973Z",
          "iopub.execute_input": "2024-01-14T15:46:06.599497Z",
          "iopub.status.idle": "2024-01-14T15:46:06.751982Z",
          "shell.execute_reply.started": "2024-01-14T15:46:06.59946Z",
          "shell.execute_reply": "2024-01-14T15:46:06.750392Z"
        },
        "trusted": true,
        "id": "CvwjGct_paVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "\n",
        "plt.subplot(121)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_hashtags, palette='crest_r')\n",
        "plt.title('Top 10 Common Hashtags')\n",
        "plt.xlabel('Hashtags')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(122)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_mentions, palette='flare_r')\n",
        "plt.title('Top 10 Common Mentions')\n",
        "plt.xlabel('Mentions')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:46:11.153624Z",
          "iopub.execute_input": "2024-01-14T15:46:11.154125Z",
          "iopub.status.idle": "2024-01-14T15:46:12.055306Z",
          "shell.execute_reply.started": "2024-01-14T15:46:11.154087Z",
          "shell.execute_reply": "2024-01-14T15:46:12.054098Z"
        },
        "trusted": true,
        "id": "VR0tHFDWpaVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/twitter_%23%40.png\"\n",
        "     width=\"300000000\" />\n",
        "     \n",
        "`#MKR` (@mykitchenrules) is the most popular hashtag in the dataset whereas `@MaxBlumenthal` is the most popular mention.\n"
      ],
      "metadata": {
        "id": "vEd8IzczpaVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I did some digging around `@MaxBlumenthal` and found this:\n",
        "\n",
        "#### Who he is:\n",
        "- American author and blogger: Born in 1977, Blumenthal has written for publications like The Nation, The New York Times, and The Daily Beast. He currently edits The Grayzone website.\n",
        "- Investigative journalist: Blumenthal's work focuses on politics, media, and human rights, often with a critical view of US foreign policy and the Israeli-Palestinian conflict.\n",
        "- Author: He's written several books, including \"Republican Gomorrah\" and \"Goliath: Life and Loathing in Greater Israel,\" which won awards and ignited debate.\n",
        "\n",
        "#### Controversies:\n",
        "- The Grayzone: Some see The Grayzone as promoting conspiracy theories and Russian propaganda, particularly regarding the Syrian Civil War. Blumenthal maintains editorial independence and focuses on investigative journalism.\n",
        "- Methodology: Critics question Blumenthal's use of anonymous sources and selective evidence, while supporters defend his investigative techniques and access to information denied to mainstream media."
      ],
      "metadata": {
        "id": "DsJ8cJZ-paVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.subplot(221)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_sexist_hashtags, palette='crest_r')\n",
        "plt.title('Top 10 Common Sexist Hashtags')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(222)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_racist_hashtags, palette='crest_r')\n",
        "plt.title('Top 10 Common Racist Hashtags')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(223)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_sexist_mentions, palette='flare_r')\n",
        "plt.title('Top 10 Common Sexist Mentions')\n",
        "plt.xlabel('Mention')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplot(224)\n",
        "sns.barplot(x='Label', y='Count', data=top_10_racist_mentions, palette='flare_r')\n",
        "plt.title('Top 10 Common Racist Mentions')\n",
        "plt.xlabel('Mention')\n",
        "plt.ylabel('')\n",
        "plt.xticks(rotation=90)\n",
        "add_spines(linewidth=1)\n",
        "\n",
        "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.15, hspace=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:46:15.972041Z",
          "iopub.execute_input": "2024-01-14T15:46:15.972569Z",
          "iopub.status.idle": "2024-01-14T15:46:17.475644Z",
          "shell.execute_reply.started": "2024-01-14T15:46:15.972528Z",
          "shell.execute_reply": "2024-01-14T15:46:17.474669Z"
        },
        "trusted": true,
        "id": "RPbvyNK5paVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Key takeaways from the most common @ and #s\n",
        "\n",
        "- For some reason `#MKR` and `#mkr` (my kitchen rules) feature on the top of the sexist hashtags. Maybe coz its a cooking show. lol. It might be due to the fact that #MKR is the most common hashtag on the dataset. <br>\n",
        "- You can see religiously motived terrorist groups like `#isis` feature on the most common racist hashtags <br>\n",
        "- `@YesYoureSexist` sounds like a provocative username. No wonder it has the second most sexist mentions\n",
        "- `@MaxBlumenthal` a controversial American author and blogger features on the top of Racist mentions"
      ],
      "metadata": {
        "id": "P9e0MFzppaVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the text"
      ],
      "metadata": {
        "id": "yOsMHpLTpaVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', tweet)\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)\n",
        "\n",
        "    # Remove 'RT' (Retweet) indicator\n",
        "    tweet = re.sub(r'\\bRT\\b', '', tweet)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     tweet_tokens = nltk.word_tokenize(tweet)\n",
        "#     tweet = ' '.join([word for word in tweet_tokens if word not in stop_words])\n",
        "\n",
        "    # Lemmatization\n",
        "    doc = nlp(tweet)\n",
        "    # Lemmatize each token and join them back into a string\n",
        "    tweet = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "    return tweet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:46:22.533875Z",
          "iopub.execute_input": "2024-01-14T15:46:22.534412Z",
          "iopub.status.idle": "2024-01-14T15:46:22.544343Z",
          "shell.execute_reply.started": "2024-01-14T15:46:22.534374Z",
          "shell.execute_reply": "2024-01-14T15:46:22.542696Z"
        },
        "trusted": true,
        "id": "DjXvZDtDpaVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text'] = df['Text'].apply(clean_tweet)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:46:31.264574Z",
          "iopub.execute_input": "2024-01-14T15:46:31.265132Z",
          "iopub.status.idle": "2024-01-14T15:49:15.52656Z",
          "shell.execute_reply.started": "2024-01-14T15:46:31.265092Z",
          "shell.execute_reply": "2024-01-14T15:49:15.5251Z"
        },
        "trusted": true,
        "id": "wSlP9dhepaVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('_________Sample clean tweets_________')\n",
        "simple_tweet = df[df['Label'] == 0]['Text'].iloc[1]\n",
        "print('\\n\\nSimple tweet: ', simple_tweet)\n",
        "sexist_tweet = df[df['Annotation'] == 'sexism']['Text'].iloc[0]\n",
        "print('\\n\\nSexist tweet: ', sexist_tweet)\n",
        "racist_tweet = df[df['Annotation'] == 'racism']['Text'].iloc[0]\n",
        "print('\\n\\nRacist tweet: ', racist_tweet)\n",
        "print('\\n\\n_______________________________')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T15:49:15.528724Z",
          "iopub.execute_input": "2024-01-14T15:49:15.529221Z",
          "iopub.status.idle": "2024-01-14T15:49:15.554648Z",
          "shell.execute_reply.started": "2024-01-14T15:49:15.529184Z",
          "shell.execute_reply": "2024-01-14T15:49:15.553275Z"
        },
        "trusted": true,
        "id": "JD-Bl7rbpaVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚òÅÔ∏è Wordclouds for toxic text <a class=\"anchor\"  id=\"section_1_3\"></a>\n",
        "\n",
        "We are using a popular method of plotting wordclouds in the shapes of masks created out of png images. Lets say you want to create a wordcloud for text related to sexism...\n",
        "1. Simply google search for an image like 'sexism logo png'\n",
        "2. Choose an image of your choice, left click on it and open it in a new tab <br>\n",
        "3. Copy the url and paste it in the code hidden below\n",
        "\n",
        "For a detailed look at how to do this.. check out this notebook: [Spooky NLP and Topic Modelling tutorial](https://www.kaggle.com/code/arthurtok/spooky-nlp-and-topic-modelling-tutorial/notebook) <br>\n",
        "This notebook uses the Base64 encoding of images to create the wordclouds while you can straightaway use the URL of the images as I have done below\n",
        "Viola!! there you have it. Beautiful wordclouds with meanings represented in the form of shapes. The hidden code cell below explains all the steps."
      ],
      "metadata": {
        "id": "VuGlZSbOpaVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopword=set(STOPWORDS)\n",
        "plt.figure(figsize=(15,40))\n",
        "\n",
        "## Common words in all bullying tweets-----------------------------------------------------------------------------------\n",
        "plt.subplot(131)\n",
        "# 1. Get text\n",
        "bully_text = df[df.Label==1.0][\"Text\"].values\n",
        "Text = ''\n",
        "for text in bully_text:\n",
        "    Text += text\n",
        "\n",
        "# 2. Load png image from url and create mask\n",
        "image_url = 'https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/bullying-computer-icons-harassment-clip-art-bully.jpg'\n",
        "response = requests.get(image_url, stream=True)\n",
        "response.raise_for_status()\n",
        "mask = np.array(Image.open(response.raw))\n",
        "\n",
        "# 3. Plot wordcloud\n",
        "wc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n",
        "     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\n",
        "plt.axis(\"off\")\n",
        "plt.title('Bullying tweets')\n",
        "plt.imshow(wc.recolor(colormap= 'summer' , random_state=244), alpha=0.98)\n",
        "\n",
        "\n",
        "## Common words in all racist tweets-----------------------------------------------------------------------------------\n",
        "# Repeat for other plots\n",
        "plt.subplot(132)\n",
        "bully_text = df[df.Annotation=='racism'][\"Text\"].values\n",
        "Text = ''\n",
        "for text in bully_text:\n",
        "    Text += text\n",
        "\n",
        "image_url = 'https://clipart-library.com/img1/1475559.png'\n",
        "response = requests.get(image_url, stream=True)\n",
        "response.raise_for_status()\n",
        "mask = np.array(Image.open(response.raw))\n",
        "\n",
        "wc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n",
        "     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\n",
        "plt.title('Racist tweets')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n",
        "\n",
        "\n",
        "## Common words in all sexist tweets-----------------------------------------------------------------------------------\n",
        "plt.subplot(133)\n",
        "bully_text = df[df.Annotation=='sexism'][\"Text\"].values\n",
        "Text = ''\n",
        "for text in bully_text:\n",
        "    Text += text\n",
        "\n",
        "image_url = 'https://raw.githubusercontent.com/harshjadhav890/cyberbullying_detection/main/woman-gender-symbol-male-female-text-line-circle-number-png-clipart.jpg'\n",
        "response.raise_for_status()\n",
        "response = requests.get(image_url, stream=True)\n",
        "mask = np.array(Image.open(response.raw))\n",
        "\n",
        "wc = WordCloud(background_color = 'black', mask = mask, contour_width = 2,\n",
        "     contour_color = 'black', colormap = 'BuPu_r', width = 800, height = 800, stopwords = stopword).generate(Text)\n",
        "plt.axis(\"off\")\n",
        "plt.title('Sexist tweets')\n",
        "plt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-14T15:49:15.556111Z",
          "iopub.execute_input": "2024-01-14T15:49:15.556513Z",
          "iopub.status.idle": "2024-01-14T15:49:35.238878Z",
          "shell.execute_reply.started": "2024-01-14T15:49:15.556481Z",
          "shell.execute_reply": "2024-01-14T15:49:35.237718Z"
        },
        "trusted": true,
        "id": "L3sWfSs1paVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üëæ Modelling <a class=\"anchor\"  id=\"chapter2\"></a>\n",
        "\n",
        "### Preprocessing for modelling\n",
        "Most of these steps have been covered earlier. You may expand this section to view the rest of the changes I made to the dataset"
      ],
      "metadata": {
        "id": "GkJQh8EMpaVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore this. I did this for ease of processing when i restart the kernel\n",
        "# this part gives you the completely preprocessed data right to the point where you just need to oversample and split it\n",
        "\n",
        "df = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\n",
        "df = df.drop(['id', 'index'], axis=1)\n",
        "df.rename(columns={'oh_label': 'Label'}, inplace=True)\n",
        "df = df.dropna(subset=['Label'])\n",
        "\n",
        "# Adding hashtags and mentions_____________________________________________________________________________________#\n",
        "# here we create a function to count the number of hashtags and mentions and then create two columns to store this data\n",
        "def count_symbols(text):\n",
        "#     hashtag_count = len(re.findall(r'#', text))\n",
        "    mention_count = len(re.findall(r'@', text))\n",
        "#     return hashtag_count, mention_count\n",
        "    return mention_count\n",
        "\n",
        "# Apply the function to each row in the 'text' column\n",
        "df[['num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n",
        "# df[['num_hashtags', 'num_mentions']] = df['Text'].apply(lambda x: pd.Series(count_symbols(x)))\n",
        "\n",
        "\n",
        "# Analyze sentiment of each text____________________________________________________________________________________#\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "def get_sentiment(text):\n",
        "    compound_score = sia.polarity_scores(text)['compound']\n",
        "    return 'positive' if compound_score >= 0 else 'negative'\n",
        "\n",
        "# Apply the function to each row in the 'Text' column\n",
        "df['Sentiment_Label'] = df['Text'].apply(get_sentiment)\n",
        "\n",
        "\n",
        "# Cleaning the tweets_________________________________________________________________________________________________#\n",
        "def clean_tweet(tweet):\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', tweet)\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)\n",
        "\n",
        "    # Remove 'RT' (Retweet) indicator\n",
        "    tweet = re.sub(r'\\bRT\\b', '', tweet)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Lemmatization\n",
        "    doc = nlp(tweet)\n",
        "    # Lemmatize each token and join them back into a string\n",
        "    tweet = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "    return tweet\n",
        "\n",
        "df['Text'] = df['Text'].apply(clean_tweet)\n",
        "#____________________________________________________________________________________________________________________#\n",
        "\n",
        "# One hot encoding of the sentiment category\n",
        "one_hot_encoded = pd.get_dummies(df['Sentiment_Label'], prefix='sentiment')\n",
        "df = pd.concat([df, one_hot_encoded], axis=1)\n",
        "df = df.drop('Sentiment_Label', axis=1)\n",
        "df['Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "df['Text'] = df['Text'].str.strip()\n",
        "\n",
        "# Step 2: Upsample the data using RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df['Text'].values.reshape(-1, 1)\n",
        "y = df['Label'].values\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# convert text (object) data to string for w2v\n",
        "X_resampled= [str(obj) for obj in X_resampled]\n",
        "X_resampled = np.array(X_resampled)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-14T18:05:23.290251Z",
          "iopub.execute_input": "2024-01-14T18:05:23.291347Z",
          "iopub.status.idle": "2024-01-14T18:07:31.990924Z",
          "shell.execute_reply.started": "2024-01-14T18:05:23.291312Z",
          "shell.execute_reply": "2024-01-14T18:07:31.989952Z"
        },
        "trusted": true,
        "id": "Jjl_h9tkpaVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Oversample the data using RandomOverSampler to reduce imbalance between Labels (Bullying and non bullying)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df['Text'].values.reshape(-1, 1)\n",
        "y = df['Label'].values\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# convert text (object) data to string for w2v\n",
        "X_resampled= [str(obj) for obj in X_resampled]\n",
        "X_resampled = np.array(X_resampled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:07:31.992813Z",
          "iopub.execute_input": "2024-01-14T18:07:31.993179Z",
          "iopub.status.idle": "2024-01-14T18:07:32.562676Z",
          "shell.execute_reply.started": "2024-01-14T18:07:31.993147Z",
          "shell.execute_reply": "2024-01-14T18:07:32.56178Z"
        },
        "trusted": true,
        "id": "8nq3UHnhpaVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML approach with word2vec <a class=\"anchor\"  id=\"section_2_1\"></a>\n",
        "#### Why use Word2Vec?\n",
        "- Semantic understanding: Word2Vec captures the semantic relationships between words, which is crucial for understanding the nuances of language used in bullying. It can differentiate between words with similar spellings but different meanings (e.g., \"joke\" vs. \"mock\") and identify synonyms and antonyms, helping to detect subtle insults and sarcasm often employed in cyberbullying.\n",
        "\n",
        "- Handling slang and abbreviations: Twitter language is full of slang, abbreviations, and emojis. Word2Vec can learn these informal and often-evolving expressions, leading to more accurate detection of bullying even when it doesn't use traditional language patterns.\n",
        "\n",
        "\n",
        "#### Limitations of using Word2Vec\n",
        "- Bias: Word2Vec models can inherit biases from the training data, potentially leading to discriminatory outcomes. Careful selection of training data and evaluation methods are crucial.\n",
        "- Context sensitivity: Subtle forms of bullying often rely on context not captured solely by word meaning. Combining Word2Vec with other approaches that consider context can improve accuracy.\n",
        "\n",
        "For detailed explaination of the intuition behind W2V read: [Word2Vec Explained](https://archive.is/4lJS5)"
      ],
      "metadata": {
        "id": "U4DLhWQupaVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec Model\n",
        "sentences = [word_tokenize(text) for text in X_resampled]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)  # Adjust parameters as needed\n",
        "\n",
        "# Convert Text to Embeddings\n",
        "def get_embedding(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    # Filter out tokens that are not in the vocabulary\n",
        "    tokens = [token for token in tokens if token in word2vec_model.wv.key_to_index]\n",
        "    if len(tokens) > 0:\n",
        "        # Return the average of word embeddings for the tokens\n",
        "        return np.mean([word2vec_model.wv[t] for t in tokens], axis=0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Create an array of embeddings for each text\n",
        "X_resampled = [get_embedding(text) for text in X_resampled]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T16:30:04.334549Z",
          "iopub.execute_input": "2024-01-14T16:30:04.334917Z",
          "iopub.status.idle": "2024-01-14T16:30:14.97919Z",
          "shell.execute_reply.started": "2024-01-14T16:30:04.334885Z",
          "shell.execute_reply": "2024-01-14T16:30:14.978095Z"
        },
        "trusted": true,
        "id": "cJhjlAilpaVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Model Training (Using a RandomForestClassifier as an example)\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T16:30:14.980454Z",
          "iopub.execute_input": "2024-01-14T16:30:14.980822Z",
          "iopub.status.idle": "2024-01-14T16:30:56.658852Z",
          "shell.execute_reply.started": "2024-01-14T16:30:14.980792Z",
          "shell.execute_reply": "2024-01-14T16:30:56.657444Z"
        },
        "trusted": true,
        "id": "Il8Jm9nypaVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pycaret's AutoML <a class=\"anchor\"  id=\"section_2_1_1\"></a>"
      ],
      "metadata": {
        "id": "2ocNYKjHpaVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -Uqqq pycaret==3.1.0\n",
        "# from pycaret.classification import *\n",
        "\n",
        "# Train-Test Split: ensure you are using the word2vec processed data from the above section\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# create a dataset using X_train and y_train\n",
        "columns = []\n",
        "dataset = np.column_stack((X_train, y_train))\n",
        "for i in range(300):\n",
        "    columns.append(str(i))\n",
        "columns.append('Label')\n",
        "auto_df = pd.DataFrame(dataset, columns=columns)\n",
        "\n",
        "auto_df.head()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "aGSFikGhpaVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pycaret_automl = setup(data=auto_df, target = 'Label', session_id=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T16:37:46.724763Z",
          "iopub.execute_input": "2024-01-14T16:37:46.725273Z",
          "iopub.status.idle": "2024-01-14T16:37:51.561979Z",
          "shell.execute_reply.started": "2024-01-14T16:37:46.72523Z",
          "shell.execute_reply": "2024-01-14T16:37:51.560869Z"
        },
        "trusted": true,
        "id": "CEZjU-QOpaVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = compare_models()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T16:37:54.022292Z",
          "iopub.execute_input": "2024-01-14T16:37:54.022841Z",
          "iopub.status.idle": "2024-01-14T17:02:40.282644Z",
          "shell.execute_reply.started": "2024-01-14T16:37:54.022797Z",
          "shell.execute_reply": "2024-01-14T17:02:40.281402Z"
        },
        "trusted": true,
        "id": "5K9lFS_0paVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Deep learning models <a class=\"anchor\"  id=\"section_2_2\"></a>"
      ],
      "metadata": {
        "id": "rgbuRvcjpaVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the cleaned version of the dataset\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df['Text'].values.reshape(-1, 1)\n",
        "y = df['Label'].values\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "X_resampled = X_resampled[:, 0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:07:40.67535Z",
          "iopub.execute_input": "2024-01-14T18:07:40.675735Z",
          "iopub.status.idle": "2024-01-14T18:07:40.690065Z",
          "shell.execute_reply.started": "2024-01-14T18:07:40.675704Z",
          "shell.execute_reply": "2024-01-14T18:07:40.68934Z"
        },
        "trusted": true,
        "id": "g1IZ1xOJpaV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using keras tokenizer to find the length vocab\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 40\n",
        "\n",
        "token.fit_on_texts(list(X_train) + list(X_test))\n",
        "xtrain_seq = token.texts_to_sequences(X_train)\n",
        "xvalid_seq = token.texts_to_sequences(X_test)\n",
        "\n",
        "#zero pad the sequences\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "\n",
        "word_index = token.word_index"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:07:43.474164Z",
          "iopub.execute_input": "2024-01-14T18:07:43.474914Z",
          "iopub.status.idle": "2024-01-14T18:07:44.399606Z",
          "shell.execute_reply.started": "2024-01-14T18:07:43.474874Z",
          "shell.execute_reply": "2024-01-14T18:07:44.398605Z"
        },
        "trusted": true,
        "id": "aeFlpxU3paV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Preprocessed text', X_train[1])\n",
        "print('\\nTokenized text', xtrain_seq[1])\n",
        "print('\\nPadded text', xtrain_pad[1])\n",
        "print('\\nPadded text Length: ', len(xtrain_pad[1]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:07:47.134264Z",
          "iopub.execute_input": "2024-01-14T18:07:47.134975Z",
          "iopub.status.idle": "2024-01-14T18:07:47.140897Z",
          "shell.execute_reply.started": "2024-01-14T18:07:47.134942Z",
          "shell.execute_reply": "2024-01-14T18:07:47.139878Z"
        },
        "trusted": true,
        "id": "gNljedTMpaV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the GloVe vectors in a dictionary:\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray([float(val) for val in values[1:]])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:07:47.399946Z",
          "iopub.execute_input": "2024-01-14T18:07:47.400256Z",
          "iopub.status.idle": "2024-01-14T18:11:32.026435Z",
          "shell.execute_reply.started": "2024-01-14T18:07:47.40023Z",
          "shell.execute_reply": "2024-01-14T18:11:32.025458Z"
        },
        "trusted": true,
        "id": "eFmRUXawpaV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an embedding matrix for the words we have in the dataset\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:11:32.028078Z",
          "iopub.execute_input": "2024-01-14T18:11:32.028377Z",
          "iopub.status.idle": "2024-01-14T18:11:32.090792Z",
          "shell.execute_reply.started": "2024-01-14T18:11:32.02835Z",
          "shell.execute_reply": "2024-01-14T18:11:32.089929Z"
        },
        "trusted": true,
        "id": "6FwY1XVspaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ôæÔ∏è Simple RNNs with trainable embeddings <a class=\"anchor\"  id=\"section_2_2_1\"></a>"
      ],
      "metadata": {
        "id": "hLOS6fhYpaV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     input_length=max_len))\n",
        "    model.add(SimpleRNN(100))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:11:52.984914Z",
          "iopub.execute_input": "2024-01-14T18:11:52.985925Z",
          "iopub.status.idle": "2024-01-14T18:11:53.781685Z",
          "shell.execute_reply.started": "2024-01-14T18:11:52.985891Z",
          "shell.execute_reply": "2024-01-14T18:11:53.779721Z"
        },
        "trusted": true,
        "id": "UBllB6nVpaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:11:53.783266Z",
          "iopub.execute_input": "2024-01-14T18:11:53.783602Z",
          "iopub.status.idle": "2024-01-14T18:13:17.189413Z",
          "shell.execute_reply.started": "2024-01-14T18:11:53.783552Z",
          "shell.execute_reply": "2024-01-14T18:13:17.188409Z"
        },
        "trusted": true,
        "id": "-_gviLIppaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,y_test)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:42.252308Z",
          "iopub.execute_input": "2024-01-14T18:13:42.253519Z",
          "iopub.status.idle": "2024-01-14T18:13:42.91689Z",
          "shell.execute_reply.started": "2024-01-14T18:13:42.253471Z",
          "shell.execute_reply": "2024-01-14T18:13:42.915991Z"
        },
        "trusted": true,
        "id": "ZSX6RMgHpaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåê LSTM with GloVe embeddings <a class=\"anchor\"  id=\"section_2_2_2\"></a>\n",
        "\n",
        "\n",
        "#### Advantages of GloVe Embeddings:\n",
        "- Global Matrix Factorization: GloVe considers global word-word co-occurrence statistics, capturing broader semantic relationships and potentially enhancing bullying detection accuracy.\n",
        "- Efficient Training: GloVe's training algorithm is computationally efficient, allowing for analysis of large tweet datasets more easily.\n",
        "\n",
        "#### Limitations of GloVe Embeddings\n",
        "- Bias: As with any word embedding model, GloVe can inherit biases from training data. Careful selection and evaluation are crucial to mitigate discriminatory outcomes.\n",
        "- Context Sensitivity: While GloVe captures global semantic relationships, it may miss subtle nuances that rely heavily on context. Combining it with context-aware techniques is recommended for optimal results.\n",
        "\n",
        "For detailed explaination of the intuition behind GloVe read: [NLP ‚Äî Word Embedding & GloVe](https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6)"
      ],
      "metadata": {
        "id": "ZjrnWLtcpaV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "\n",
        "    # A simple LSTM with glove embeddings and one dense layer\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "\n",
        "    model.add(LSTM(100))\n",
        "    # dropout=0.3, recurrent_dropout=0.3\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:43.566201Z",
          "iopub.execute_input": "2024-01-14T18:13:43.56654Z",
          "iopub.status.idle": "2024-01-14T18:13:43.923329Z",
          "shell.execute_reply.started": "2024-01-14T18:13:43.566513Z",
          "shell.execute_reply": "2024-01-14T18:13:43.922447Z"
        },
        "trusted": true,
        "id": "fJK7of1dpaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:43.925137Z",
          "iopub.execute_input": "2024-01-14T18:13:43.925926Z",
          "iopub.status.idle": "2024-01-14T18:13:53.130256Z",
          "shell.execute_reply.started": "2024-01-14T18:13:43.92589Z",
          "shell.execute_reply": "2024-01-14T18:13:53.129324Z"
        },
        "trusted": true,
        "id": "MimOkwt9paV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,y_test)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:53.131882Z",
          "iopub.execute_input": "2024-01-14T18:13:53.132198Z",
          "iopub.status.idle": "2024-01-14T18:13:53.933288Z",
          "shell.execute_reply.started": "2024-01-14T18:13:53.13217Z",
          "shell.execute_reply": "2024-01-14T18:13:53.932443Z"
        },
        "trusted": true,
        "id": "8WDAHVGfpaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRUs with trainable embeddings <a class=\"anchor\"  id=\"section_2_2_3\"></a>"
      ],
      "metadata": {
        "id": "uUhyJg1WpaV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    # GRU with glove embeddings and two dense layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     input_length=max_len))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(GRU(300))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:53.934425Z",
          "iopub.execute_input": "2024-01-14T18:13:53.934781Z",
          "iopub.status.idle": "2024-01-14T18:13:54.221358Z",
          "shell.execute_reply.started": "2024-01-14T18:13:53.934753Z",
          "shell.execute_reply": "2024-01-14T18:13:54.220508Z"
        },
        "trusted": true,
        "id": "8FAfy8UxpaV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain_pad, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:13:54.224241Z",
          "iopub.execute_input": "2024-01-14T18:13:54.224612Z",
          "iopub.status.idle": "2024-01-14T18:14:37.081718Z",
          "shell.execute_reply.started": "2024-01-14T18:13:54.224561Z",
          "shell.execute_reply": "2024-01-14T18:14:37.080757Z"
        },
        "trusted": true,
        "id": "zCErqyc5paV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.predict(xvalid_pad)\n",
        "print(\"Auc: %.2f%%\" % (roc_auc(scores,y_test)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.08294Z",
          "iopub.execute_input": "2024-01-14T18:14:37.083233Z",
          "iopub.status.idle": "2024-01-14T18:14:37.798826Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.083209Z",
          "shell.execute_reply": "2024-01-14T18:14:37.797998Z"
        },
        "trusted": true,
        "id": "fq_4a6NfpaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "We can see that trainable embeddings perform better than GloVe embeddings on our task. GloVe embeddings do a good job of providing us with global embeddings. These embeddings are not contextually aware and hence are not well suited for complex tasks. eg. The word 'date' may have different meanings depending on the context."
      ],
      "metadata": {
        "id": "fvEU_4dPpaV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚¨ÖÔ∏è‚û°Ô∏è BERT with Tensorflow Hub <a class=\"anchor\"  id=\"section_2_3\"></a>"
      ],
      "metadata": {
        "id": "GAHPk5ESpaV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df['Text'].values.reshape(-1, 1)\n",
        "y = df['Label'].values\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "X_resampled = X_resampled[:, 0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.799821Z",
          "iopub.execute_input": "2024-01-14T18:14:37.800077Z",
          "iopub.status.idle": "2024-01-14T18:14:37.814741Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.800055Z",
          "shell.execute_reply": "2024-01-14T18:14:37.814Z"
        },
        "trusted": true,
        "id": "JrQcFioHpaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating tensorflow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test))\n",
        "\n",
        "BUFFER_SIZE = 4000\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.815716Z",
          "iopub.execute_input": "2024-01-14T18:14:37.816Z",
          "iopub.status.idle": "2024-01-14T18:14:37.839431Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.815967Z",
          "shell.execute_reply": "2024-01-14T18:14:37.838739Z"
        },
        "trusted": true,
        "id": "aRiZ_FAcpaV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dataset element spec:\", train_dataset.element_spec)\n",
        "print(\"Test dataset element spec:\", test_dataset.element_spec)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.840388Z",
          "iopub.execute_input": "2024-01-14T18:14:37.840677Z",
          "iopub.status.idle": "2024-01-14T18:14:37.845311Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.840653Z",
          "shell.execute_reply": "2024-01-14T18:14:37.844502Z"
        },
        "trusted": true,
        "id": "1ATFE_0KpaV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1): # take returns a batch of train_dataset, containing 64 feature, label pairs.\n",
        "    print('texts: ', example.numpy()[:3])\n",
        "    print()\n",
        "    print('labels: ', label.numpy()[:3])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.84635Z",
          "iopub.execute_input": "2024-01-14T18:14:37.846646Z",
          "iopub.status.idle": "2024-01-14T18:14:37.872826Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.846619Z",
          "shell.execute_reply": "2024-01-14T18:14:37.871914Z"
        },
        "trusted": true,
        "id": "xqByszp3paV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to use a tf hub BERT model <a class=\"anchor\"  id=\"section_2_3_1\"></a>\n",
        "To use and train a BERT model from tensorflow hub, you simply need the links from where the model will be downloaded and trained. <br>\n",
        "Every BERT model requires a BERT preprocess model that takes care of all the preprocessing required to input data in to BERT."
      ],
      "metadata": {
        "id": "JEOZ4yzypaV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the preprocessing and main model from tf hub links\n",
        "\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:37.875839Z",
          "iopub.execute_input": "2024-01-14T18:14:37.876189Z",
          "iopub.status.idle": "2024-01-14T18:14:49.406332Z",
          "shell.execute_reply.started": "2024-01-14T18:14:37.876162Z",
          "shell.execute_reply": "2024-01-14T18:14:49.405589Z"
        },
        "trusted": true,
        "id": "GkkHAbfspaV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we take some sample input texts and test the bert preprocess model on them. We analyze the outputs given by the model below"
      ],
      "metadata": {
        "id": "L96U6OSspaV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the bert preprocessor\n",
        "\n",
        "text_test = [\"Today's weather is harsh\", \"My name is harsh\"]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "for i in range(len(text_test)):\n",
        "    print('Input text :', text_test[i])\n",
        "    print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "    print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "    print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][i, :12]}')\n",
        "    print(f'Input Mask : {text_preprocessed[\"input_mask\"][i, :12]}')\n",
        "    print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][i, :12]}')\n",
        "    print('\\n')\n",
        "# input_type_ids has the same shape as input_mask, but inside the non-padded region,\n",
        "# contains a 0 or a 1 indicating which sentence the token is a part of.\n",
        "\n",
        "# context is not being mapped at the preprocessing layer\n",
        "# The word 'Harsh' will have the same token no matter what the scenario\n",
        "# BERT will perform the contextualization when the text is passed through it"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:49.407567Z",
          "iopub.execute_input": "2024-01-14T18:14:49.407891Z",
          "iopub.status.idle": "2024-01-14T18:14:49.61943Z",
          "shell.execute_reply.started": "2024-01-14T18:14:49.407866Z",
          "shell.execute_reply": "2024-01-14T18:14:49.618592Z"
        },
        "trusted": true,
        "id": "WcAIThCGpaV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model link: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
        "\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Keys       : {list(bert_results.keys())}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Encoder Outputs Shape:{bert_results[\"encoder_outputs\"][0].shape}')\n",
        "# print(f'Encoder Outputs Values:{bert_results[\"encoder_outputs\"][0][0, :3]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "# print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :3]}')\n",
        "\n",
        "# BERT takes our sentence and outputs a 768 token pooled representation, where every word is mapped in a 128 dimension array"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:49.620748Z",
          "iopub.execute_input": "2024-01-14T18:14:49.621434Z",
          "iopub.status.idle": "2024-01-14T18:14:49.87878Z",
          "shell.execute_reply.started": "2024-01-14T18:14:49.621397Z",
          "shell.execute_reply": "2024-01-14T18:14:49.877837Z"
        },
        "trusted": true,
        "id": "L4bBlhQ8paV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The pooled outputs provide a fixed-size representation of the entire input sequence,\n",
        "- The encoder outputs represent the contextualized embeddings of each token, and the sequence\n",
        "- Outputs retain token-level information for more detailed analysis or tasks.\n",
        "- Either of those can be used as input to further model building depending on granularity of the task"
      ],
      "metadata": {
        "id": "RiQtGW3rpaV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that takes in the links for the model and returns a newly compiled model\n",
        "\n",
        "def create_model(model_link, preprocess_link):\n",
        "  # bert_model = hub.KerasLayer(model_link)\n",
        "  # bert_preprocess_model = hub.KerasLayer(preprcess_link)\n",
        "\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(preprocess_link, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(model_link, trainable=True, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "\n",
        "    model = tf.keras.Model(text_input, net)\n",
        "\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metrics = tf.metrics.BinaryAccuracy()\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                              loss=loss,\n",
        "                              metrics=metrics)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:49.880101Z",
          "iopub.execute_input": "2024-01-14T18:14:49.880762Z",
          "iopub.status.idle": "2024-01-14T18:14:49.888025Z",
          "shell.execute_reply.started": "2024-01-14T18:14:49.880731Z",
          "shell.execute_reply": "2024-01-14T18:14:49.887033Z"
        },
        "trusted": true,
        "id": "fHB9hu6gpaV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SIZE = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "\n",
        "print(\"Size of the dataset:\", DATA_SIZE)\n",
        "BATCH_SIZE = 16\n",
        "STEPS_PER_EPOCH = DATA_SIZE // BATCH_SIZE"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:49.889476Z",
          "iopub.execute_input": "2024-01-14T18:14:49.889881Z",
          "iopub.status.idle": "2024-01-14T18:14:49.900337Z",
          "shell.execute_reply.started": "2024-01-14T18:14:49.889846Z",
          "shell.execute_reply": "2024-01-14T18:14:49.899427Z"
        },
        "trusted": true,
        "id": "wuV98_PZpaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_en_uncased_L-4_H-256_A-4/2\n",
        "\n",
        "with strategy.scope():\n",
        "    model = create_model('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2', 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "model.fit(train_dataset, epochs=10, batch_size=16)\n",
        "# directly passed in the train_dataset tensor to the model as the preprocessing layer takes care of embedding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:14:49.901477Z",
          "iopub.execute_input": "2024-01-14T18:14:49.902243Z",
          "iopub.status.idle": "2024-01-14T18:26:00.273559Z",
          "shell.execute_reply.started": "2024-01-14T18:14:49.90221Z",
          "shell.execute_reply": "2024-01-14T18:26:00.27265Z"
        },
        "trusted": true,
        "id": "AhVFGt--paWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since the model is showing continued improvements let us train it some more\n",
        "model.fit(train_dataset, epochs=5, batch_size=16)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:26:00.274916Z",
          "iopub.execute_input": "2024-01-14T18:26:00.275301Z",
          "iopub.status.idle": "2024-01-14T18:31:15.878951Z",
          "shell.execute_reply.started": "2024-01-14T18:26:00.275266Z",
          "shell.execute_reply": "2024-01-14T18:31:15.878058Z"
        },
        "trusted": true,
        "id": "Q0in4Bk0paWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've achieved respectable results using BERT, let us save the model locally for future inference."
      ],
      "metadata": {
        "id": "bEeWQQJYpaWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/kaggle/working/twitter_smallbert_en_uncased_L-4_H-256_A-4/2'\n",
        "model.save(model_path, include_optimizer=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:31:15.880159Z",
          "iopub.execute_input": "2024-01-14T18:31:15.880467Z",
          "iopub.status.idle": "2024-01-14T18:31:23.097298Z",
          "shell.execute_reply.started": "2024-01-14T18:31:15.880441Z",
          "shell.execute_reply": "2024-01-14T18:31:23.096492Z"
        },
        "trusted": true,
        "id": "jlZ6krpdpaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload\n",
        "reloaded_model = tf.saved_model.load(model_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:31:23.098864Z",
          "iopub.execute_input": "2024-01-14T18:31:23.099166Z",
          "iopub.status.idle": "2024-01-14T18:31:30.733359Z",
          "shell.execute_reply.started": "2024-01-14T18:31:23.099141Z",
          "shell.execute_reply": "2024-01-14T18:31:30.732535Z"
        },
        "trusted": true,
        "id": "5nEmx3hSpaWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# infer\n",
        "def print_my_examples(inputs, results):\n",
        "  result_for_printing = \\\n",
        "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
        "                         for i in range(len(inputs))]\n",
        "  print(*result_for_printing, sep='\\n')\n",
        "  print()\n",
        "\n",
        "\n",
        "examples = [\n",
        "    'Your mom is a fucking whore',\n",
        "    'You are beautiful',\n",
        "    'suck my dick you fucking bitch',\n",
        "    'all muslims are terrorists',\n",
        "    'Women are terrible drivers'\n",
        "]\n",
        "\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
        "original_results = tf.sigmoid(model(tf.constant(examples)))\n",
        "\n",
        "print('Results from the saved model:')\n",
        "print_my_examples(examples, reloaded_results)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:31:30.734771Z",
          "iopub.execute_input": "2024-01-14T18:31:30.735121Z",
          "iopub.status.idle": "2024-01-14T18:31:31.45567Z",
          "shell.execute_reply.started": "2024-01-14T18:31:30.735087Z",
          "shell.execute_reply": "2024-01-14T18:31:31.454755Z"
        },
        "trusted": true,
        "id": "AMomTCakpaWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "You can see that all the tweets except the second one get a high score. Showing that our model is doing good."
      ],
      "metadata": {
        "id": "bLTRJO-GpaWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets download the file from our kaggle working directory\n",
        "\n",
        "# You can use !zip -r filename.zip directory_path command to zip the directory.\n",
        "!zip -r twitter_smallbert_en_uncased_L-4_H-256_A-4.zip /kaggle/working/twitter_smallbert_en_uncased_L-4_H-256_A-4\n",
        "\n",
        "from IPython.display import FileLink\n",
        "FileLink(r'/kaggle/working/twitter_smallbert_en_uncased_L-4_H-256_A-4.zip')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-14T18:31:31.457076Z",
          "iopub.execute_input": "2024-01-14T18:31:31.457932Z",
          "iopub.status.idle": "2024-01-14T18:31:37.578004Z",
          "shell.execute_reply.started": "2024-01-14T18:31:31.457893Z",
          "shell.execute_reply": "2024-01-14T18:31:37.576906Z"
        },
        "trusted": true,
        "id": "FwCeXaSupaWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}